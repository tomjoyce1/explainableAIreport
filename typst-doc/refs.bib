%===============================================================================
% Sample bibliopgraphy file for ifaconf.bst style to be used in
% IFAC meeting papers
% Copyright (c) 2007-2008 International Federation of Automatic Control
%===============================================================================

@incollection{Lajewska2024,
    author={W. Łajewska and K. Balog},
    title={Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations},
    booktitle={Lecture Notes in Computer Science},
    publisher={Springer},
    year={2024},
    pages={317--328},
    address={Cham},
    note={ECIR 2024; Chapter 25 in Vol. 14597. Available: https://link.springer.com/chapter/10.1007/978-3-031-56063-7_25},
}

@article{Hallucination2025,
    author={{AI Hallucinations? What About Human Hallucination?! Addressing Human Imperfection Is Needed for an Ethical AI}},
    title={AI Hallucinations? What About Human Hallucination?! Addressing Human Imperfection Is Needed for an Ethical AI},
    journal={International Journal of Interactive Multimedia and Artificial Intelligence (IJIMAI)},
    year={2025},
    volume={9},
    number={2},
    pages={8--18},
    note={Web of Science WOS:001436441200008; Available: https://www.ijimai.org/journal/sites/default/files/2025-02/ijimai9_2_7.pdf},
}

@misc{Sanderson2024Impact,
    author={A. Sanderson et al.},
    title={Evaluating the Impact of Hallucinations on User Trust and Reliance on AI Systems},
    year={2024},
    note={Online. Available: https://www.diva-portal.org/smash/get/diva2:1870904/FULLTEXT01.pdf},
}

@misc{Understanding2024,
    author={{Understanding the Effects of Miscalibrated AI Confidence on Human Trust}},
    title={Understanding the Effects of Miscalibrated AI Confidence on Human Trust},
    year={2024},
    note={arXiv:2402.07632. Available: https://arxiv.org/html/2402.07632v4},
}

@inproceedings{Wang2023,
    author={D. Wang et al.},
    title={Measuring and Understanding Trust Calibrations for Explainable AI},
    booktitle={Proc. ACM Conf.},
    year={2023},
    note={Available: https://dl.acm.org/doi/fullHtml/10.1145/3544548.3581197},
}

@misc{Lai2020,
    author={V. Lai et al.},
    title={Effect of Confidence and Explanation on Accuracy and Trust Calibration in Human-AI Collaboration},
    year={2020},
    note={arXiv:2001.02114. Available: https://arxiv.org/abs/2001.02114},
}

@misc{FakeIt2025,
    author={{Fake it till you make it? AI hallucinations and ethical responsibility in medicine}},
    title={Fake it till you make it? AI hallucinations and ethical responsibility in medicine},
    year={2025},
    note={PubMed Central. Available: https://pmc.ncbi.nlm.nih.gov/articles/PMC12237199/. Accessed: Oct. 6, 2025.},
}

@misc{Feather2025,
    author={{AI Hallucinations in Healthcare: Understanding Risks and Responsibilities}},
    title={AI Hallucinations in Healthcare: Understanding Risks and Responsibilities},
    year={2025},
    note={Feather. Available: https://www.askfeather.com/resources/ai-hallucinations-in-healthcare. Accessed: Oct. 6, 2025.},
}

@article{Transparency2024,
    author={{Transparency and accountability in AI systems}},
    title={Transparency and accountability in AI systems},
    journal={Frontiers in Human Dynamics},
    year={2024},
    note={Available: https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2024.1421273/full. Accessed: Oct. 6, 2025.},
}

@misc{Legal500_2025,
    author={{AI Hallucinations: When Creation Comes at a Cost, Who Pays?}},
    title={AI Hallucinations: When Creation Comes at a Cost, Who Pays?},
    year={2025},
    note={The Legal 500. Available: https://www.legal500.com/developments/thought-leadership/ai-hallucinations-when-creation-comes-at-a-cost-who-pays/. Accessed: Oct. 6, 2025.},
}

@article{Massenon2025,
    author={R. Massenon et al.},
    title={"My AI is Lying to Me": User-reported LLM hallucinations in AI mobile apps reviews},
    journal={Sci Rep.},
    year={2025},
    volume={15},
    pages={30397},
    doi={10.1038/s41598-025-15416-8},
    note={Available: https://pmc.ncbi.nlm.nih.gov/articles/PMC12365265/},
}

@article{Vasconcelos2023,
    author={H. Vasconcelos et al.},
    title={Explanations Can Reduce Overreliance on AI Systems During Decision-Making},
    journal={Proc. ACM Hum.-Comput. Interact. (CSCW)},
    year={2023},
    volume={7},
    number={CSCW1},
    pages={1--38},
    note={Available: https://hci.stanford.edu/publications/2023/xai-cscw-2023.pdf},
}

@inproceedings{Zhang2020,
    author={Y. Zhang, Q. Liao, and R. K. E. Bellamy},
    title={Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making},
    booktitle={FAccT '20},
    year={2020},
    doi={10.1145/3351095.3372852},
    note={Available: https://dl.acm.org/doi/abs/10.1145/3351095.3372852},
}

@article{Naiseh2021,
    author={M. Naiseh et al.},
    title={Explainable Recommendations and Calibrated Trust: Two Systematic User Errors},
    journal={Computer (IEEE)},
    year={2021},
    volume={54},
    number={10},
    pages={28--37},
    doi={10.1109/MC.2021.3076131},
    note={Available: https://doi.org/10.1109/MC.2021.3076131},
}

@inproceedings{Bender2021,
    author={E. M. Bender et al.},
    title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
    booktitle={FAccT '21},
    year={2021},
    doi={10.1145/3442188.3445922},
    note={Available: https://dl.acm.org/doi/10.1145/3442188.3445922},
}

@misc{Simhi2025,
    author={A. Simhi et al.},
    title={Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs},
    year={2025},
    note={arXiv:2502.12964. Available: https://arxiv.org/html/2502.12964v1},
}

@inproceedings{Sun2023,
    author={S. Sun, A. Kulkarni, and B. Y. Lim},
    title={"Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction},
    booktitle={CHI '23 (Best Paper HM)},
    year={2023},
    note={Available: https://sunniesuhyoung.github.io/XAI_Trust/},
}

@techreport{Berkeley2023,
    author={{The Berkeley Human-AI Teaming Capstone Research Group}},
    title={Human-AI Teaming and Trust Calibration},
    institution={Berkeley iSchool},
    year={2023},
    type={Capstone Report},
    note={Available: https://www.ischool.berkeley.edu/sites/default/files/sproject_attachments/humanai_capstonereport-final.pdf},
}

@incollection{Lajewska2024_2,
    author={W. Łajewska and K. Balog},
    title={Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations},
    booktitle={Lecture Notes in Computer Science},
    publisher={Springer},
    year={2024},
    note={Chapter 25 in Vol. 14597. Available: https://link.springer.com/chapter/10.1007/978-3-031-56063-7_25},
}

@misc{Su2024,
    author={W. Su et al.},
    title={Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models},
    year={2024},
    note={arXiv:2403.06448. Available: https://arxiv.org/pdf/2403.06448},
}

@article{Kim2024,
    author={H. Kim and S. W. Lee},
    title={INVESTIGATING THE EFFECTS OF GENERATIVE-AI RESPONSES ON USER EXPERIENCE AFTER AI HALLUCINATION},
    year={2024},
     note = {Available at: \url{https://grdspublishing.org/index.php/people/article/view/2308}}

}

@article{Sun2024Hallucination,
    author={Y. Sun et al.},
    title={AI hallucination: towards a comprehensive classification of distorted information in artificial intelligence-generated content},
    year={2024},
  note = {Available at: \url{https://www.nature.com/articles/s41599-024-03811-x}}
}

@article{Jin2024,
    author={L. Jin et al.},
    title={Exploring the determinants and effects of artificial intelligence (AI) hallucination exposure on generative AI adoption in healthcare},
    year={2024},
      note = {Available at: \url{https://journals.sagepub.com/doi/abs/10.1177/02666669251340954}}
}

@article{Massenon2025_2,
    author={R. Massenon et al.},
    title={"My AI is Lying to Me": User-reported LLM hallucinations in AI mobile apps reviews},
    journal={SCIENTIFIC REPORTS},
    year={2025},
    volume={15},
    number={1},
    pages={30397},
    doi={10.1038/s41598-025-15416-8},
      note = {Available at: \url{https://www.nature.com/articles/s41598-025-15416-8}}
}
@article{Ji2023,
    author={Z. Ji et al.},
    title={Survey of Hallucination in Large Language Models},
    journal={ACM Computing Surveys},
    year={2023},
      note = {Available at: \url{https://dl.acm.org/doi/10.1145/3703155}}
}

@inproceedings{Maynez2020,
    author={J. Maynez et al.},
    title={Faithful to the Original: Factuality and Attribution in Abstractive Summarization},
    booktitle={Proc. of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
    year={2020},
      note = {Available at: \url{https://aclanthology.org/2020.acl-main.173/}}
}

@article{Kazemi2023,
    author={S. M. Kazemi et al.},
    title={Hallucinations in Large Language Models: A Survey},
    journal={arXiv preprint},
    year={2023},
      note = {Available at: \url{https://arxiv.org/abs/2311.05232}}
}

@article{Nori2023,
    author={H. Nori et al.},
    title={Capabilities of GPT-4 in Medical and Clinical Domains},
    journal={NPJ Digital Medicine},
    year={2023},
  note = {Available at: \url{https://arxiv.org/abs/2303.13375}}
  }

@article{Zuccon2023,
    author={G. Zuccon},
    title={ChatGPT and Scholarly Hallucinations},
    journal={Nature},
    year={2023},
  note = {Available at: \url{https://www.nature.com/articles/s41599-024-03811-x}}}

@article{HoangEtAl,
    author={Q. Hoang et al.},
    title={A Classification of Hallucinations in Large Language Models},
    journal={ArXiv Preprint},
    year={2024},
  note = {Available at: \url{https://arxiv.org/abs/2311.05232}}}

@inproceedings{Reynolds2021,
    author={L. Reynolds and J. McDonell},
    title={Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
    booktitle={NeurIPS Workshop on Distribution Shifts},
    year={2021},
  note = {Available at: \url{https://arxiv.org/abs/2005.00661}}}
@article{Zhou2022,
  author = {S. Zhou et al.},
  title = {Controllable Text Generation with Language Models},
  journal = {ArXiv Preprint},
  year = {2022},
  note = {Available at: \url{https://arxiv.org/abs/2205.10625}}
}

@article{Wei2022,
  author = {J. Wei et al.},
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2022},
  note = {Available at: \url{https://arxiv.org/abs/2201.11903}}
}

@article{Bang2023,
  author = {Y. Bang and E. Madotto},
  title = {A Categorization of Hallucination Types in LLMs},
  journal = {ArXiv Preprint},
  year = {2023},
  note = {Available at: \url{https://arxiv.org/abs/2305.04552}}
}

@article{Chen2023,
  author = {T. Chen et al.},
  title = {Survey on Hallucination in Large Language Models},
  journal = {ArXiv Preprint},
  year = {2023},
  note = {Available at: \url{https://arxiv.org/abs/2305.04552}}
}

@misc{OpenAI2023a,
  author = {OpenAI},
  title = {GPT-4 Technical Report},
  year = {2023},
  note = {Available at: \url{https://openai.com/research/gpt-4}}
}

@inproceedings{Wu2023,
  author = {J. Wu et al.},
  title = {HaluEval: A Universal Evaluation Benchmark for Hallucination in Large Language Models},
  booktitle = {Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2023},
  note = {Available at: \url{https://arxiv.org/abs/2305.04552}}
}

@misc{Taori2023Hallucination,
  author = {R. Taori et al.},
  title = {Understanding Hallucinations in GPT Models},
  year = {2023},
  note = {Available at: \url{https://arxiv.org/abs/2305.04552}}
}
@misc{gpt5hALLUC,
  author = {OpenAI gpt 5 model interaction},
  title = {gpt5 Hallucination - https://imgur.com/NYKDNOQ},
  year = {2025},
  note = {Online. Available: https://imgur.com/NYKDNOQ}
}

